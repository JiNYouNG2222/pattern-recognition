{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlLXUXj62H/GuuDURgQsTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiNYouNG2222/pattern-recognition/blob/main/final_train_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "bBbBHOukh6Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_classes = 26\n",
        "epochs = 30\n",
        "Drp = 0.5"
      ],
      "metadata": {
        "id": "IfDTepu1pbSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torchvision.datasets.EMNIST(\n",
        "    root = './data/EMNIST',\n",
        "    split = 'letters',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        "test_set = torchvision.datasets.EMNIST(\n",
        "    root = './data/EMNIST',\n",
        "    split = 'letters',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        "\n",
        "print(train_set, test_set)\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "jIPzY9f3pd9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 정의\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
        "        )\n",
        "\n",
        "        # Layer 2\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
        "        )\n",
        "\n",
        "        # Layer 3\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),  # Batch Normalization\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # 7x7 -> 3x3\n",
        "        )\n",
        "\n",
        "        # Layer 4\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),  # Batch Normalization\n",
        "            nn.ReLU()\n",
        "            # No pooling, keeps 3x3 spatial dimension\n",
        "        )\n",
        "\n",
        "        # Dropout and Fully Connected Layers\n",
        "        self.dropout = nn.Dropout(p=Drp)\n",
        "        self.fc1 = nn.Linear(in_features=128 * 3 * 3, out_features=1000)\n",
        "        self.fc2 = nn.Linear(in_features=1000, out_features=26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KEQXezvApS4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화, 손실 함수 및 최적화 기법 설정\n",
        "net = NeuralNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# 훈련 루프\n",
        "pd_results = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = net(images)\n",
        "        loss = criterion(out, labels-1)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total = labels.size(0)\n",
        "        preds = torch.max(out.data, 1)[1]\n",
        "        correct = (preds==labels-1).sum().item()\n",
        "\n",
        "        if (i+1)%200==0:\n",
        "            results = OrderedDict()\n",
        "            results['epoch'] = epoch+1\n",
        "            results['idx'] = i+1\n",
        "            results['loss'] = loss.item()\n",
        "            results['accuracy'] = 100.*correct/total\n",
        "            pd_results.append(results)\n",
        "            df = pd.DataFrame.from_dict(pd_results, orient='columns')\n",
        "\n",
        "            clear_output(wait=True)\n",
        "            # display(df)\n",
        "            print(df)\n"
      ],
      "metadata": {
        "id": "toMAJ96wo83j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = net(images)\n",
        "        preds = torch.max(out.data, 1)[1]\n",
        "        correct += (preds==labels-1).sum().item()\n",
        "        total += len(labels)\n",
        "\n",
        "    print(\"Test accuracy: \", 100.*correct/total)"
      ],
      "metadata": {
        "id": "Ip2KWJ3ho8-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), \"jy_ver1.pth\")"
      ],
      "metadata": {
        "id": "8PWgc9QHo9Gv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}